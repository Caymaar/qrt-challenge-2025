{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14024, 12)\n",
      "(14024, 15)\n",
      "(14024, 17)\n",
      "Index(['gene_ASXL1', 'gene_BCOR', 'gene_BCORL1', 'gene_BRCC3', 'gene_CBL',\n",
      "       'gene_CEBPA', 'gene_CSF3R', 'gene_CTCF', 'gene_CUX1', 'gene_DDX41',\n",
      "       ...\n",
      "       'SKEW(molecular.START)', 'SKEW(molecular.VAF)', 'STD(molecular.DEPTH)',\n",
      "       'STD(molecular.END)', 'STD(molecular.START)', 'STD(molecular.VAF)',\n",
      "       'SUM(molecular.DEPTH)', 'SUM(molecular.END)', 'SUM(molecular.START)',\n",
      "       'SUM(molecular.VAF)'],\n",
      "      dtype='object', length=102)\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import create_entity, predict_and_save, split_data, get_method_name\n",
    "from src.preprocess import process_missing_values, main_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Régler le logger de Featuretools au niveau ERROR\n",
    "logging.getLogger('featuretools.entityset').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\".*Ill-conditioned matrix.*\")\n",
    "\n",
    "data = create_entity()\n",
    "\n",
    "GLOBAL = {\n",
    "    \"save_cox\": False,\n",
    "    \"save_xgb\": True,\n",
    "    \"save_lgbm\": False,\n",
    "    \"save_rsf\": True\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    \"size\": 0.7,\n",
    "    \"clinical\": [\"CYTOGENETICS\"],#[\"CYTOGENETICS\"], # Possible: [\"CYTOGENETICS\", \"HB/PLT\", \"logMONOCYTES\", \"logWBC\", \"logANC\"]\n",
    "    \"molecular\": [\"GENE\"],#[\"END-START\"], # Possible: [\"GENE\", \"EFFECT\", \"ALT\", \"REF\", \"END-START\"]\n",
    "    \"merge\": [\"featuretools\", \"gpt\"], # Possible: [\"featuretools\", \"gpt\"]\n",
    "    \"rsf\": {\n",
    "    'n_estimators':150,  # Nombre d'arbres dans la forêt\n",
    "    'min_samples_split':50,  # Nombre minimum d'échantillons requis pour splitter un nœud\n",
    "    'min_samples_leaf':20,  # Nombre minimum d'échantillons par feuille\n",
    "    'max_features':\"sqrt\",  # Sélection aléatoire des features\n",
    "    'n_jobs':-1,  # Utilisation de tous les cœurs disponibles\n",
    "    }\n",
    "}\n",
    "\n",
    "data = main_preprocess(data, PARAMS['clinical'], PARAMS['molecular'], PARAMS['merge'])\n",
    "X, X_eval, y = split_data(data)\n",
    "# Check if there are any columns that are not float or int in X\n",
    "print(X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 - PARAMS['size']), random_state=42)\n",
    "\n",
    "X_train, X_test, X_eval = process_missing_values(X_train, X_test, X_eval, method=\"impute\", strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf_params_method = \"_\".join([(str(key) + \"=\" + str(PARAMS['rsf'][key])) for key in PARAMS['rsf'].keys()])\n",
    "\n",
    "rsf = RandomSurvivalForest(min_samples_split=PARAMS['rsf']['min_samples_split'], n_estimators=PARAMS['rsf']['n_estimators'], min_samples_leaf=PARAMS['rsf']['min_samples_leaf'], max_features=PARAMS['rsf']['max_features'], n_jobs=PARAMS['rsf']['n_jobs'], random_state=42)\n",
    "rsf.fit(X_train, y_train)\n",
    "rsf_cindex_train = concordance_index_ipcw(y_train, y_train, rsf.predict(X_train), tau=7)[0]\n",
    "rsf_cindex_test = concordance_index_ipcw(y_train, y_test, rsf.predict(X_test), tau=7)[0]\n",
    "print(f\"Random Survival Forest Model Concordance Index IPCW on train: {rsf_cindex_train:.3f}\")\n",
    "print(f\"Random Survival Forest Model Concordance Index IPCW on test: {rsf_cindex_test:.3f}\")\n",
    "rsf_score_method = f\"score_{rsf_cindex_train:.3f}_{rsf_cindex_test:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.675\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.675\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.675\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.675\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.675\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.675\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.678\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.712, C-index (test) = 0.688\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.712, C-index (test) = 0.688\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.712, C-index (test) = 0.688\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.712, C-index (test) = 0.688\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.712, C-index (test) = 0.688\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.712, C-index (test) = 0.688\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.692\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.680\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.699, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.680\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.699, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.680\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.699, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.680\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.699, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.680\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.699, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.697, C-index (test) = 0.677\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.698, C-index (test) = 0.680\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.699, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.700, C-index (test) = 0.681\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.701, C-index (test) = 0.682\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.711, C-index (test) = 0.689\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.722, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.711, C-index (test) = 0.689\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.722, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.711, C-index (test) = 0.689\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.722, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.711, C-index (test) = 0.689\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.722, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.711, C-index (test) = 0.689\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.722, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "✅ C-index (train) = 0.711, C-index (test) = 0.689\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "✅ C-index (train) = 0.719, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "✅ C-index (train) = 0.721, C-index (test) = 0.693\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "✅ C-index (train) = 0.720, C-index (test) = 0.694\n",
      "🔄 Entraînement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "✅ C-index (train) = 0.718, C-index (test) = 0.693\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# 🚀 Étape 1: Transformer les données en format sksurv\n",
    "#y_train_surv = Surv.from_dataframe(event=\"OS_STATUS\", time=\"OS_YEARS\", data=y_train)\n",
    "#y_test_surv = Surv.from_dataframe(event=\"OS_STATUS\", time=\"OS_YEARS\", data=y_test)\n",
    "\n",
    "# 🚀 Étape 2: Définir une grille d'hyperparamètres à tester\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300],  # Nombre d'arbres\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"min_samples_split\": [2, 5, 10, 30, 50, 70],  # Nombre minimum d'échantillons pour split\n",
    "    \"min_samples_leaf\": [2, 5, 10, 30, 50, 70],  # Nombre minimum d'échantillons par feuille\n",
    "    \"max_features\": [None],  # Nombre de features utilisées pour chaque split\n",
    "}\n",
    "\n",
    "# 🚀 Étape 3: Tester toutes les combinaisons d'hyperparamètres\n",
    "results = []\n",
    "\n",
    "for params in product(*param_grid.values()):\n",
    "    param_dict = dict(zip(param_grid.keys(), params))\n",
    "    print(f\"🔄 Entraînement avec {param_dict}...\")\n",
    "\n",
    "    # Initialisation du modèle avec les paramètres actuels\n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_estimators=param_dict[\"n_estimators\"],\n",
    "        max_depth=param_dict['max_depth'],\n",
    "        min_samples_split=param_dict[\"min_samples_split\"],\n",
    "        min_samples_leaf=param_dict[\"min_samples_leaf\"],\n",
    "        max_features=param_dict[\"max_features\"],\n",
    "        n_jobs=-1,  # Utiliser tous les cœurs CPU\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    rsf.fit(X_train, y_train)\n",
    "\n",
    "    # Prédiction et évaluation\n",
    "    rsf_cindex_train = concordance_index_ipcw(y_train, y_train, rsf.predict(X_train), tau=7)[0]\n",
    "    rsf_cindex_test = concordance_index_ipcw(y_train, y_test, rsf.predict(X_test), tau=7)[0]\n",
    "    \n",
    "    print(f\"✅ C-index (train) = {rsf_cindex_train:.3f}, C-index (test) = {rsf_cindex_test:.3f}\")\n",
    "\n",
    "    # Stocker les résultats\n",
    "    results.append({\n",
    "        \"n_estimators\": param_dict[\"n_estimators\"],\n",
    "        \"min_samples_split\": param_dict[\"min_samples_split\"],\n",
    "        \"min_samples_leaf\": param_dict[\"min_samples_leaf\"],\n",
    "        \"max_features\": param_dict[\"max_features\"],\n",
    "        \"cindex_train\": rsf_cindex_train,\n",
    "        \"cindex_test\": rsf_cindex_test,\n",
    "    })\n",
    "\n",
    "# 🚀 Étape 4: Convertir les résultats en DataFrame pour analyse\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# 📊 Afficher les résultats triés par la meilleure performance\n",
    "df_results = df_results.sort_values(by=\"cindex_test\", ascending=False)\n",
    "##mport ace_tools as tools\n",
    "#tools.display_dataframe_to_user(name=\"RSF Hyperparameter Tuning Results\", dataframe=df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('rsf_hyperparameter_tuning_results3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>cindex_train</th>\n",
       "      <th>cindex_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.857146</td>\n",
       "      <td>0.711642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.880996</td>\n",
       "      <td>0.711488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.883259</td>\n",
       "      <td>0.711273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.859483</td>\n",
       "      <td>0.711166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.710305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.856244</td>\n",
       "      <td>0.696440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.819768</td>\n",
       "      <td>0.695889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.832458</td>\n",
       "      <td>0.695834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.848412</td>\n",
       "      <td>0.694599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.848412</td>\n",
       "      <td>0.694599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  min_samples_split  min_samples_leaf max_features  \\\n",
       "20            50                 20                 2         None   \n",
       "11            50                 10                 2         None   \n",
       "38           100                 10                 2         None   \n",
       "47           100                 20                 2         None   \n",
       "2             50                  5                 2         None   \n",
       "..           ...                ...               ...          ...   \n",
       "3             50                  5                 5         sqrt   \n",
       "22            50                 20                 5         log2   \n",
       "19            50                 20                 2         log2   \n",
       "13            50                 10                 5         log2   \n",
       "4             50                  5                 5         log2   \n",
       "\n",
       "    cindex_train  cindex_test  \n",
       "20      0.857146     0.711642  \n",
       "11      0.880996     0.711488  \n",
       "38      0.883259     0.711273  \n",
       "47      0.859483     0.711166  \n",
       "2       0.898407     0.710305  \n",
       "..           ...          ...  \n",
       "3       0.856244     0.696440  \n",
       "22      0.819768     0.695889  \n",
       "19      0.832458     0.695834  \n",
       "13      0.848412     0.694599  \n",
       "4       0.848412     0.694599  \n",
       "\n",
       "[81 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# 📌 Étape 1 : Générer des interactions de manière optimisée\n",
    "def generate_interactions(df, max_combinations=2):\n",
    "    \"\"\"\n",
    "    Génère des interactions entre les variables de df en utilisant différentes opérations.\n",
    "    \n",
    "    max_combinations : int, nombre maximal de features à combiner\n",
    "    \"\"\"\n",
    "    df_interactions = df.copy()\n",
    "    feature_pairs = list(itertools.combinations(df.columns, max_combinations))\n",
    "    \n",
    "    # 📌 Stocker les nouvelles colonnes dans un dictionnaire pour une concaténation efficace\n",
    "    new_features = {}\n",
    "\n",
    "    for f1, f2 in feature_pairs:\n",
    "        new_features[f\"{f1}_mul_{f2}\"] = df[f1] * df[f2]  # Multiplication\n",
    "        new_features[f\"{f1}_add_{f2}\"] = df[f1] + df[f2]  # Addition\n",
    "        new_features[f\"{f1}_div_{f2}\"] = df[f1] / (df[f2] + 1e-6)  # Division sécurisée\n",
    "        new_features[f\"log_{f1}\"] = np.log(df[f1] + 1)  # Log transformation\n",
    "    \n",
    "    # 📌 Ajouter toutes les nouvelles colonnes en une seule fois\n",
    "    df_interactions = pd.concat([df_interactions, pd.DataFrame(new_features)], axis=1)\n",
    "    \n",
    "    return df_interactions\n",
    "\n",
    "# 📌 Étape 2 : Sélectionner les interactions avec SHAP\n",
    "def select_best_interactions(X_train, y_train, model_params, top_k=10):\n",
    "    \"\"\"\n",
    "    Sélectionne les interactions les plus importantes en utilisant SHAP et un modèle de Gradient Boosting.\n",
    "    \"\"\"\n",
    "    y_train_surv = Surv.from_dataframe(event=\"event\", time=\"time\", data=y_train)\n",
    "\n",
    "    # Réduire le nombre de features avant SHAP\n",
    "    top_features = X_train.corrwith(y_train[\"time\"]).abs().nlargest(100).index\n",
    "    X_train = X_train[top_features]\n",
    "\n",
    "    # Entraînement du modèle GBSA (Gradient Boosting Survival Analysis)\n",
    "    model = GradientBoostingSurvivalAnalysis(**model_params)\n",
    "    model.fit(X_train, y_train_surv)\n",
    "\n",
    "    def predict_function(data):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = pd.DataFrame(data, columns=X_train.columns)\n",
    "        return model.predict(data)\n",
    "\n",
    "    # 📌 Utilisation de TreeExplainer au lieu de PermutationExplainer\n",
    "    explainer = shap.Explainer(predict_function, X_train)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    # 📌 Importance des features\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    feature_importance = pd.Series(shap_importance, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "    # 📌 Sélection des meilleures interactions\n",
    "    best_features = feature_importance.head(top_k).index.tolist()\n",
    "    return X_train[best_features], feature_importance, shap_values\n",
    "\n",
    "# 📌 Étape 3 : Pipeline Complet\n",
    "def process_features_with_interactions(X_train, y_train, model_params, max_combinations=2, top_k=10):\n",
    "    \"\"\"\n",
    "    Pipeline complet qui génère et sélectionne automatiquement les meilleures interactions avec SHAP.\n",
    "    \"\"\"\n",
    "    print(\"🛠️ Génération des interactions...\")\n",
    "    X_train_interactions = generate_interactions(X_train, max_combinations)\n",
    "\n",
    "    print(f\"🔍 Sélection des {top_k} meilleures interactions avec SHAP...\")\n",
    "    X_train_selected = select_best_interactions(X_train_interactions, y_train, model_params, top_k)\n",
    "\n",
    "    return X_train_selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BM_BLAST', 'WBC', 'ANC', 'MONOCYTES', 'HB', 'PLT', 'num_subclones',\n",
      "       'sex', 'avg_chromosomes', 'total_mitoses', 'num_translocations',\n",
      "       'num_deletions', 'num_inversions', 'num_duplications', 'num_additions',\n",
      "       'num_monosomies', 'num_trisomies', 'complexity_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import create_entity, predict_and_save, split_data, get_method_name\n",
    "from src.preprocess import process_missing_values, main_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Régler le logger de Featuretools au niveau ERROR\n",
    "logging.getLogger('featuretools.entityset').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\".*Ill-conditioned matrix.*\")\n",
    "\n",
    "data = create_entity()\n",
    "\n",
    "GLOBAL = {\n",
    "    \"save_cox\": False,\n",
    "    \"save_xgb\": True,\n",
    "    \"save_lgbm\": False,\n",
    "    \"save_rsf\": True\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    \"size\": 0.7,\n",
    "    \"clinical\": [\"CYTOGENETICS\"],#[\"CYTOGENETICS\"], # Possible: [\"CYTOGENETICS\", \"HB/PLT\", \"logMONOCYTES\", \"logWBC\", \"logANC\"]\n",
    "    \"molecular\": [],#[\"GENE\"],#[\"END-START\"], # Possible: [\"GENE\", \"EFFECT\", \"ALT\", \"REF\", \"END-START\"]\n",
    "    \"merge\": [],#[\"featuretools\", \"gpt\"], # Possible: [\"featuretools\", \"gpt\"]\n",
    "    \"xgb\": {\n",
    "        'max_depth': 2,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 260,\n",
    "        'subsample': 1,\n",
    "        'max_features': None,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "}\n",
    "\n",
    "data = main_preprocess(data, PARAMS['clinical'], PARAMS['molecular'], PARAMS['merge'])\n",
    "X, X_eval, y = split_data(data)\n",
    "# Check if there are any columns that are not float or int in X\n",
    "print(X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 - PARAMS['size']), random_state=42)\n",
    "\n",
    "X_train, X_test, X_eval = process_missing_values(X_train, X_test, X_eval, method=\"impute\", strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Génération des interactions...\n",
      "🔍 Sélection des 10 meilleures interactions avec SHAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 2222it [01:52, 17.90it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nouvelles variables sélectionnées :\n",
      "   BM_BLAST_add_WBC  BM_BLAST_div_HB  HB_mul_PLT  HB_div_num_trisomies  \\\n",
      "0               6.5         0.329670      1365.0          9.100000e+06   \n",
      "1              19.0         1.363636       495.0          1.100000e+07   \n",
      "2               8.7         0.869565       910.8          6.900000e+06   \n",
      "3               4.0         0.200000      1780.0          1.000000e+07   \n",
      "4              12.0         1.000000       530.0          4.999998e+00   \n",
      "\n",
      "   PLT_add_total_mitoses  PLT_div_num_monosomies  HB_div_num_monosomies  \\\n",
      "0                  150.0            1.500000e+08           9.100000e+06   \n",
      "1                   65.0            4.500000e+07           1.100000e+07   \n",
      "2                  162.0            1.320000e+08           6.900000e+06   \n",
      "3                  197.0            1.779998e+02           9.999990e+00   \n",
      "4                   78.0            2.649999e+01           4.999998e+00   \n",
      "\n",
      "   BM_BLAST_add_MONOCYTES  PLT_div_complexity_score  sex_add_num_trisomies  \n",
      "0                    4.05              1.500000e+08                    1.0  \n",
      "1                   15.00              4.500000e+07                    1.0  \n",
      "2                    6.17              1.319999e+02                    1.0  \n",
      "3                    2.00              1.780000e+08                    1.0  \n",
      "4                   10.00              1.766666e+01                    2.0  \n"
     ]
    }
   ],
   "source": [
    "model_params = PARAMS['xgb']\n",
    "\n",
    "X_train_new, feature_importance, shap_values = process_features_with_interactions(pd.DataFrame(X_train, columns=X.columns), pd.DataFrame(y_train), model_params, max_combinations=2, top_k=10)\n",
    "\n",
    "print(\"✅ Nouvelles variables sélectionnées :\")\n",
    "print(X_train_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BM_BLAST_add_WBC                      0.168483\n",
       "BM_BLAST_div_HB                       0.117520\n",
       "HB_mul_PLT                            0.104916\n",
       "HB_div_num_trisomies                  0.103098\n",
       "PLT_add_total_mitoses                 0.060907\n",
       "                                        ...   \n",
       "num_inversions_add_num_monosomies     0.000000\n",
       "BM_BLAST_add_num_additions            0.000000\n",
       "BM_BLAST_add_num_trisomies            0.000000\n",
       "num_additions_add_num_trisomies       0.000000\n",
       "num_deletions_add_complexity_score    0.000000\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BM_BLAST_add_WBC</th>\n",
       "      <th>BM_BLAST_div_HB</th>\n",
       "      <th>HB_mul_PLT</th>\n",
       "      <th>HB_div_num_trisomies</th>\n",
       "      <th>PLT_div_num_monosomies</th>\n",
       "      <th>HB_div_num_monosomies</th>\n",
       "      <th>PLT_add_total_mitoses</th>\n",
       "      <th>PLT_div_complexity_score</th>\n",
       "      <th>BM_BLAST_add_MONOCYTES</th>\n",
       "      <th>BM_BLAST_add_ANC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.50</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>1365.0</td>\n",
       "      <td>9.100000e+06</td>\n",
       "      <td>1.500000e+08</td>\n",
       "      <td>9.100000e+06</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.500000e+08</td>\n",
       "      <td>4.050</td>\n",
       "      <td>3.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.00</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>495.0</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>4.500000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.500000e+07</td>\n",
       "      <td>15.000</td>\n",
       "      <td>17.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.70</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>910.8</td>\n",
       "      <td>6.900000e+06</td>\n",
       "      <td>1.320000e+08</td>\n",
       "      <td>6.900000e+06</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1.319999e+02</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.779998e+02</td>\n",
       "      <td>9.999990e+00</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.780000e+08</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>4.999998e+00</td>\n",
       "      <td>2.649999e+01</td>\n",
       "      <td>4.999998e+00</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.766666e+01</td>\n",
       "      <td>10.000</td>\n",
       "      <td>11.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>5.80</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>606.8</td>\n",
       "      <td>7.400000e+06</td>\n",
       "      <td>8.200000e+07</td>\n",
       "      <td>7.400000e+06</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8.199992e+01</td>\n",
       "      <td>3.220</td>\n",
       "      <td>4.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>11.90</td>\n",
       "      <td>0.707071</td>\n",
       "      <td>2316.6</td>\n",
       "      <td>9.900000e+06</td>\n",
       "      <td>2.340000e+08</td>\n",
       "      <td>9.900000e+06</td>\n",
       "      <td>254.0</td>\n",
       "      <td>2.339998e+02</td>\n",
       "      <td>7.340</td>\n",
       "      <td>9.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1896.6</td>\n",
       "      <td>1.090000e+07</td>\n",
       "      <td>1.740000e+08</td>\n",
       "      <td>1.090000e+07</td>\n",
       "      <td>194.0</td>\n",
       "      <td>1.739998e+02</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>8.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000.9</td>\n",
       "      <td>1.070000e+07</td>\n",
       "      <td>1.870000e+08</td>\n",
       "      <td>1.070000e+07</td>\n",
       "      <td>207.0</td>\n",
       "      <td>1.870000e+08</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>5.31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1702.4</td>\n",
       "      <td>1.119999e+01</td>\n",
       "      <td>1.520000e+08</td>\n",
       "      <td>1.120000e+07</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.520000e+08</td>\n",
       "      <td>0.796</td>\n",
       "      <td>2.283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2221 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BM_BLAST_add_WBC  BM_BLAST_div_HB  HB_mul_PLT  HB_div_num_trisomies  \\\n",
       "0                 6.50         0.329670      1365.0          9.100000e+06   \n",
       "1                19.00         1.363636       495.0          1.100000e+07   \n",
       "2                 8.70         0.869565       910.8          6.900000e+06   \n",
       "3                 4.00         0.200000      1780.0          1.000000e+07   \n",
       "4                12.00         1.000000       530.0          4.999998e+00   \n",
       "...                ...              ...         ...                   ...   \n",
       "2216              5.80         0.405405       606.8          7.400000e+06   \n",
       "2217             11.90         0.707071      2316.6          9.900000e+06   \n",
       "2218              3.30         0.000000      1896.6          1.090000e+07   \n",
       "2219              8.00         0.000000      2000.9          1.070000e+07   \n",
       "2220              5.31         0.000000      1702.4          1.119999e+01   \n",
       "\n",
       "      PLT_div_num_monosomies  HB_div_num_monosomies  PLT_add_total_mitoses  \\\n",
       "0               1.500000e+08           9.100000e+06                  150.0   \n",
       "1               4.500000e+07           1.100000e+07                   65.0   \n",
       "2               1.320000e+08           6.900000e+06                  162.0   \n",
       "3               1.779998e+02           9.999990e+00                  197.0   \n",
       "4               2.649999e+01           4.999998e+00                   78.0   \n",
       "...                      ...                    ...                    ...   \n",
       "2216            8.200000e+07           7.400000e+06                  102.0   \n",
       "2217            2.340000e+08           9.900000e+06                  254.0   \n",
       "2218            1.740000e+08           1.090000e+07                  194.0   \n",
       "2219            1.870000e+08           1.070000e+07                  207.0   \n",
       "2220            1.520000e+08           1.120000e+07                  172.0   \n",
       "\n",
       "      PLT_div_complexity_score  BM_BLAST_add_MONOCYTES  BM_BLAST_add_ANC  \n",
       "0                 1.500000e+08                   4.050             3.840  \n",
       "1                 4.500000e+07                  15.000            17.000  \n",
       "2                 1.319999e+02                   6.170             6.945  \n",
       "3                 1.780000e+08                   2.000             3.000  \n",
       "4                 1.766666e+01                  10.000            11.000  \n",
       "...                        ...                     ...               ...  \n",
       "2216              8.199992e+01                   3.220             4.400  \n",
       "2217              2.339998e+02                   7.340             9.890  \n",
       "2218              1.739998e+02                   0.200             1.730  \n",
       "2219              1.870000e+08                   2.000             4.080  \n",
       "2220              1.520000e+08                   0.796             2.283  \n",
       "\n",
       "[2221 rows x 10 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(itertools.combinations(X.columns, 2)))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "top_features = X_train.corrwith(y_train[\"time\"]).abs().nlargest(100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PLT', 'BM_BLAST', 'HB', 'num_monosomies', 'complexity_score',\n",
       "       'num_trisomies', 'num_additions', 'num_deletions', 'num_translocations',\n",
       "       'num_subclones', 'MONOCYTES', 'WBC', 'sex', 'num_duplications', 'ANC',\n",
       "       'avg_chromosomes', 'num_inversions', 'total_mitoses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
