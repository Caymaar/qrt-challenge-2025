{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14024, 12)\n",
      "(14024, 15)\n",
      "(14024, 17)\n",
      "Index(['gene_ASXL1', 'gene_BCOR', 'gene_BCORL1', 'gene_BRCC3', 'gene_CBL',\n",
      "       'gene_CEBPA', 'gene_CSF3R', 'gene_CTCF', 'gene_CUX1', 'gene_DDX41',\n",
      "       ...\n",
      "       'SKEW(molecular.START)', 'SKEW(molecular.VAF)', 'STD(molecular.DEPTH)',\n",
      "       'STD(molecular.END)', 'STD(molecular.START)', 'STD(molecular.VAF)',\n",
      "       'SUM(molecular.DEPTH)', 'SUM(molecular.END)', 'SUM(molecular.START)',\n",
      "       'SUM(molecular.VAF)'],\n",
      "      dtype='object', length=102)\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import create_entity, predict_and_save, split_data, get_method_name\n",
    "from src.preprocess import process_missing_values, main_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# RÃ©gler le logger de Featuretools au niveau ERROR\n",
    "logging.getLogger('featuretools.entityset').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\".*Ill-conditioned matrix.*\")\n",
    "\n",
    "data = create_entity()\n",
    "\n",
    "GLOBAL = {\n",
    "    \"save_cox\": False,\n",
    "    \"save_xgb\": True,\n",
    "    \"save_lgbm\": False,\n",
    "    \"save_rsf\": True\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    \"size\": 0.7,\n",
    "    \"clinical\": [\"CYTOGENETICS\"],#[\"CYTOGENETICS\"], # Possible: [\"CYTOGENETICS\", \"HB/PLT\", \"logMONOCYTES\", \"logWBC\", \"logANC\"]\n",
    "    \"molecular\": [\"GENE\"],#[\"END-START\"], # Possible: [\"GENE\", \"EFFECT\", \"ALT\", \"REF\", \"END-START\"]\n",
    "    \"merge\": [\"featuretools\", \"gpt\"], # Possible: [\"featuretools\", \"gpt\"]\n",
    "    \"rsf\": {\n",
    "    'n_estimators':150,  # Nombre d'arbres dans la forÃªt\n",
    "    'min_samples_split':50,  # Nombre minimum d'Ã©chantillons requis pour splitter un nÅ“ud\n",
    "    'min_samples_leaf':20,  # Nombre minimum d'Ã©chantillons par feuille\n",
    "    'max_features':\"sqrt\",  # SÃ©lection alÃ©atoire des features\n",
    "    'n_jobs':-1,  # Utilisation de tous les cÅ“urs disponibles\n",
    "    }\n",
    "}\n",
    "\n",
    "data = main_preprocess(data, PARAMS['clinical'], PARAMS['molecular'], PARAMS['merge'])\n",
    "X, X_eval, y = split_data(data)\n",
    "# Check if there are any columns that are not float or int in X\n",
    "print(X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 - PARAMS['size']), random_state=42)\n",
    "\n",
    "X_train, X_test, X_eval = process_missing_values(X_train, X_test, X_eval, method=\"impute\", strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf_params_method = \"_\".join([(str(key) + \"=\" + str(PARAMS['rsf'][key])) for key in PARAMS['rsf'].keys()])\n",
    "\n",
    "rsf = RandomSurvivalForest(min_samples_split=PARAMS['rsf']['min_samples_split'], n_estimators=PARAMS['rsf']['n_estimators'], min_samples_leaf=PARAMS['rsf']['min_samples_leaf'], max_features=PARAMS['rsf']['max_features'], n_jobs=PARAMS['rsf']['n_jobs'], random_state=42)\n",
    "rsf.fit(X_train, y_train)\n",
    "rsf_cindex_train = concordance_index_ipcw(y_train, y_train, rsf.predict(X_train), tau=7)[0]\n",
    "rsf_cindex_test = concordance_index_ipcw(y_train, y_test, rsf.predict(X_test), tau=7)[0]\n",
    "print(f\"Random Survival Forest Model Concordance Index IPCW on train: {rsf_cindex_train:.3f}\")\n",
    "print(f\"Random Survival Forest Model Concordance Index IPCW on test: {rsf_cindex_test:.3f}\")\n",
    "rsf_score_method = f\"score_{rsf_cindex_train:.3f}_{rsf_cindex_test:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.675\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.675\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.675\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.675\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.675\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.675\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.678\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.712, C-index (test) = 0.688\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.712, C-index (test) = 0.688\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.712, C-index (test) = 0.688\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.712, C-index (test) = 0.688\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.712, C-index (test) = 0.688\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.712, C-index (test) = 0.688\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.692\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.680\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.699, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.680\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.699, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.680\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.699, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.680\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.699, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.680\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.699, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.697, C-index (test) = 0.677\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.698, C-index (test) = 0.680\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.699, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.700, C-index (test) = 0.681\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 2, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.701, C-index (test) = 0.682\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.711, C-index (test) = 0.689\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.722, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.711, C-index (test) = 0.689\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.722, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.711, C-index (test) = 0.689\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.722, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.711, C-index (test) = 0.689\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.722, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.711, C-index (test) = 0.689\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.722, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 2, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.711, C-index (test) = 0.689\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 5, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.719, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 10, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 30, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.721, C-index (test) = 0.693\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 50, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.720, C-index (test) = 0.694\n",
      "ğŸ”„ EntraÃ®nement avec {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 70, 'min_samples_leaf': 70, 'max_features': None}...\n",
      "âœ… C-index (train) = 0.718, C-index (test) = 0.693\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# ğŸš€ Ã‰tape 1: Transformer les donnÃ©es en format sksurv\n",
    "#y_train_surv = Surv.from_dataframe(event=\"OS_STATUS\", time=\"OS_YEARS\", data=y_train)\n",
    "#y_test_surv = Surv.from_dataframe(event=\"OS_STATUS\", time=\"OS_YEARS\", data=y_test)\n",
    "\n",
    "# ğŸš€ Ã‰tape 2: DÃ©finir une grille d'hyperparamÃ¨tres Ã  tester\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300],  # Nombre d'arbres\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"min_samples_split\": [2, 5, 10, 30, 50, 70],  # Nombre minimum d'Ã©chantillons pour split\n",
    "    \"min_samples_leaf\": [2, 5, 10, 30, 50, 70],  # Nombre minimum d'Ã©chantillons par feuille\n",
    "    \"max_features\": [None],  # Nombre de features utilisÃ©es pour chaque split\n",
    "}\n",
    "\n",
    "# ğŸš€ Ã‰tape 3: Tester toutes les combinaisons d'hyperparamÃ¨tres\n",
    "results = []\n",
    "\n",
    "for params in product(*param_grid.values()):\n",
    "    param_dict = dict(zip(param_grid.keys(), params))\n",
    "    print(f\"ğŸ”„ EntraÃ®nement avec {param_dict}...\")\n",
    "\n",
    "    # Initialisation du modÃ¨le avec les paramÃ¨tres actuels\n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_estimators=param_dict[\"n_estimators\"],\n",
    "        max_depth=param_dict['max_depth'],\n",
    "        min_samples_split=param_dict[\"min_samples_split\"],\n",
    "        min_samples_leaf=param_dict[\"min_samples_leaf\"],\n",
    "        max_features=param_dict[\"max_features\"],\n",
    "        n_jobs=-1,  # Utiliser tous les cÅ“urs CPU\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # EntraÃ®nement du modÃ¨le\n",
    "    rsf.fit(X_train, y_train)\n",
    "\n",
    "    # PrÃ©diction et Ã©valuation\n",
    "    rsf_cindex_train = concordance_index_ipcw(y_train, y_train, rsf.predict(X_train), tau=7)[0]\n",
    "    rsf_cindex_test = concordance_index_ipcw(y_train, y_test, rsf.predict(X_test), tau=7)[0]\n",
    "    \n",
    "    print(f\"âœ… C-index (train) = {rsf_cindex_train:.3f}, C-index (test) = {rsf_cindex_test:.3f}\")\n",
    "\n",
    "    # Stocker les rÃ©sultats\n",
    "    results.append({\n",
    "        \"n_estimators\": param_dict[\"n_estimators\"],\n",
    "        \"min_samples_split\": param_dict[\"min_samples_split\"],\n",
    "        \"min_samples_leaf\": param_dict[\"min_samples_leaf\"],\n",
    "        \"max_features\": param_dict[\"max_features\"],\n",
    "        \"cindex_train\": rsf_cindex_train,\n",
    "        \"cindex_test\": rsf_cindex_test,\n",
    "    })\n",
    "\n",
    "# ğŸš€ Ã‰tape 4: Convertir les rÃ©sultats en DataFrame pour analyse\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# ğŸ“Š Afficher les rÃ©sultats triÃ©s par la meilleure performance\n",
    "df_results = df_results.sort_values(by=\"cindex_test\", ascending=False)\n",
    "##mport ace_tools as tools\n",
    "#tools.display_dataframe_to_user(name=\"RSF Hyperparameter Tuning Results\", dataframe=df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('rsf_hyperparameter_tuning_results3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>cindex_train</th>\n",
       "      <th>cindex_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.857146</td>\n",
       "      <td>0.711642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.880996</td>\n",
       "      <td>0.711488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.883259</td>\n",
       "      <td>0.711273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.859483</td>\n",
       "      <td>0.711166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.710305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.856244</td>\n",
       "      <td>0.696440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.819768</td>\n",
       "      <td>0.695889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.832458</td>\n",
       "      <td>0.695834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.848412</td>\n",
       "      <td>0.694599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.848412</td>\n",
       "      <td>0.694599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  min_samples_split  min_samples_leaf max_features  \\\n",
       "20            50                 20                 2         None   \n",
       "11            50                 10                 2         None   \n",
       "38           100                 10                 2         None   \n",
       "47           100                 20                 2         None   \n",
       "2             50                  5                 2         None   \n",
       "..           ...                ...               ...          ...   \n",
       "3             50                  5                 5         sqrt   \n",
       "22            50                 20                 5         log2   \n",
       "19            50                 20                 2         log2   \n",
       "13            50                 10                 5         log2   \n",
       "4             50                  5                 5         log2   \n",
       "\n",
       "    cindex_train  cindex_test  \n",
       "20      0.857146     0.711642  \n",
       "11      0.880996     0.711488  \n",
       "38      0.883259     0.711273  \n",
       "47      0.859483     0.711166  \n",
       "2       0.898407     0.710305  \n",
       "..           ...          ...  \n",
       "3       0.856244     0.696440  \n",
       "22      0.819768     0.695889  \n",
       "19      0.832458     0.695834  \n",
       "13      0.848412     0.694599  \n",
       "4       0.848412     0.694599  \n",
       "\n",
       "[81 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# ğŸ“Œ Ã‰tape 1 : GÃ©nÃ©rer des interactions de maniÃ¨re optimisÃ©e\n",
    "def generate_interactions(df, max_combinations=2):\n",
    "    \"\"\"\n",
    "    GÃ©nÃ¨re des interactions entre les variables de df en utilisant diffÃ©rentes opÃ©rations.\n",
    "    \n",
    "    max_combinations : int, nombre maximal de features Ã  combiner\n",
    "    \"\"\"\n",
    "    df_interactions = df.copy()\n",
    "    feature_pairs = list(itertools.combinations(df.columns, max_combinations))\n",
    "    \n",
    "    # ğŸ“Œ Stocker les nouvelles colonnes dans un dictionnaire pour une concatÃ©nation efficace\n",
    "    new_features = {}\n",
    "\n",
    "    for f1, f2 in feature_pairs:\n",
    "        new_features[f\"{f1}_mul_{f2}\"] = df[f1] * df[f2]  # Multiplication\n",
    "        new_features[f\"{f1}_add_{f2}\"] = df[f1] + df[f2]  # Addition\n",
    "        new_features[f\"{f1}_div_{f2}\"] = df[f1] / (df[f2] + 1e-6)  # Division sÃ©curisÃ©e\n",
    "        new_features[f\"log_{f1}\"] = np.log(df[f1] + 1)  # Log transformation\n",
    "    \n",
    "    # ğŸ“Œ Ajouter toutes les nouvelles colonnes en une seule fois\n",
    "    df_interactions = pd.concat([df_interactions, pd.DataFrame(new_features)], axis=1)\n",
    "    \n",
    "    return df_interactions\n",
    "\n",
    "# ğŸ“Œ Ã‰tape 2 : SÃ©lectionner les interactions avec SHAP\n",
    "def select_best_interactions(X_train, y_train, model_params, top_k=10):\n",
    "    \"\"\"\n",
    "    SÃ©lectionne les interactions les plus importantes en utilisant SHAP et un modÃ¨le de Gradient Boosting.\n",
    "    \"\"\"\n",
    "    y_train_surv = Surv.from_dataframe(event=\"event\", time=\"time\", data=y_train)\n",
    "\n",
    "    # RÃ©duire le nombre de features avant SHAP\n",
    "    top_features = X_train.corrwith(y_train[\"time\"]).abs().nlargest(100).index\n",
    "    X_train = X_train[top_features]\n",
    "\n",
    "    # EntraÃ®nement du modÃ¨le GBSA (Gradient Boosting Survival Analysis)\n",
    "    model = GradientBoostingSurvivalAnalysis(**model_params)\n",
    "    model.fit(X_train, y_train_surv)\n",
    "\n",
    "    def predict_function(data):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = pd.DataFrame(data, columns=X_train.columns)\n",
    "        return model.predict(data)\n",
    "\n",
    "    # ğŸ“Œ Utilisation de TreeExplainer au lieu de PermutationExplainer\n",
    "    explainer = shap.Explainer(predict_function, X_train)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    # ğŸ“Œ Importance des features\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    feature_importance = pd.Series(shap_importance, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "    # ğŸ“Œ SÃ©lection des meilleures interactions\n",
    "    best_features = feature_importance.head(top_k).index.tolist()\n",
    "    return X_train[best_features], feature_importance, shap_values\n",
    "\n",
    "# ğŸ“Œ Ã‰tape 3 : Pipeline Complet\n",
    "def process_features_with_interactions(X_train, y_train, model_params, max_combinations=2, top_k=10):\n",
    "    \"\"\"\n",
    "    Pipeline complet qui gÃ©nÃ¨re et sÃ©lectionne automatiquement les meilleures interactions avec SHAP.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ› ï¸ GÃ©nÃ©ration des interactions...\")\n",
    "    X_train_interactions = generate_interactions(X_train, max_combinations)\n",
    "\n",
    "    print(f\"ğŸ” SÃ©lection des {top_k} meilleures interactions avec SHAP...\")\n",
    "    X_train_selected = select_best_interactions(X_train_interactions, y_train, model_params, top_k)\n",
    "\n",
    "    return X_train_selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BM_BLAST', 'WBC', 'ANC', 'MONOCYTES', 'HB', 'PLT', 'num_subclones',\n",
      "       'sex', 'avg_chromosomes', 'total_mitoses', 'num_translocations',\n",
      "       'num_deletions', 'num_inversions', 'num_duplications', 'num_additions',\n",
      "       'num_monosomies', 'num_trisomies', 'complexity_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import create_entity, predict_and_save, split_data, get_method_name\n",
    "from src.preprocess import process_missing_values, main_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# RÃ©gler le logger de Featuretools au niveau ERROR\n",
    "logging.getLogger('featuretools.entityset').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\".*Ill-conditioned matrix.*\")\n",
    "\n",
    "data = create_entity()\n",
    "\n",
    "GLOBAL = {\n",
    "    \"save_cox\": False,\n",
    "    \"save_xgb\": True,\n",
    "    \"save_lgbm\": False,\n",
    "    \"save_rsf\": True\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    \"size\": 0.7,\n",
    "    \"clinical\": [\"CYTOGENETICS\"],#[\"CYTOGENETICS\"], # Possible: [\"CYTOGENETICS\", \"HB/PLT\", \"logMONOCYTES\", \"logWBC\", \"logANC\"]\n",
    "    \"molecular\": [],#[\"GENE\"],#[\"END-START\"], # Possible: [\"GENE\", \"EFFECT\", \"ALT\", \"REF\", \"END-START\"]\n",
    "    \"merge\": [],#[\"featuretools\", \"gpt\"], # Possible: [\"featuretools\", \"gpt\"]\n",
    "    \"xgb\": {\n",
    "        'max_depth': 2,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 260,\n",
    "        'subsample': 1,\n",
    "        'max_features': None,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "}\n",
    "\n",
    "data = main_preprocess(data, PARAMS['clinical'], PARAMS['molecular'], PARAMS['merge'])\n",
    "X, X_eval, y = split_data(data)\n",
    "# Check if there are any columns that are not float or int in X\n",
    "print(X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 - PARAMS['size']), random_state=42)\n",
    "\n",
    "X_train, X_test, X_eval = process_missing_values(X_train, X_test, X_eval, method=\"impute\", strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ GÃ©nÃ©ration des interactions...\n",
      "ğŸ” SÃ©lection des 10 meilleures interactions avec SHAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 2222it [01:52, 17.90it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Nouvelles variables sÃ©lectionnÃ©es :\n",
      "   BM_BLAST_add_WBC  BM_BLAST_div_HB  HB_mul_PLT  HB_div_num_trisomies  \\\n",
      "0               6.5         0.329670      1365.0          9.100000e+06   \n",
      "1              19.0         1.363636       495.0          1.100000e+07   \n",
      "2               8.7         0.869565       910.8          6.900000e+06   \n",
      "3               4.0         0.200000      1780.0          1.000000e+07   \n",
      "4              12.0         1.000000       530.0          4.999998e+00   \n",
      "\n",
      "   PLT_add_total_mitoses  PLT_div_num_monosomies  HB_div_num_monosomies  \\\n",
      "0                  150.0            1.500000e+08           9.100000e+06   \n",
      "1                   65.0            4.500000e+07           1.100000e+07   \n",
      "2                  162.0            1.320000e+08           6.900000e+06   \n",
      "3                  197.0            1.779998e+02           9.999990e+00   \n",
      "4                   78.0            2.649999e+01           4.999998e+00   \n",
      "\n",
      "   BM_BLAST_add_MONOCYTES  PLT_div_complexity_score  sex_add_num_trisomies  \n",
      "0                    4.05              1.500000e+08                    1.0  \n",
      "1                   15.00              4.500000e+07                    1.0  \n",
      "2                    6.17              1.319999e+02                    1.0  \n",
      "3                    2.00              1.780000e+08                    1.0  \n",
      "4                   10.00              1.766666e+01                    2.0  \n"
     ]
    }
   ],
   "source": [
    "model_params = PARAMS['xgb']\n",
    "\n",
    "X_train_new, feature_importance, shap_values = process_features_with_interactions(pd.DataFrame(X_train, columns=X.columns), pd.DataFrame(y_train), model_params, max_combinations=2, top_k=10)\n",
    "\n",
    "print(\"âœ… Nouvelles variables sÃ©lectionnÃ©es :\")\n",
    "print(X_train_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BM_BLAST_add_WBC                      0.168483\n",
       "BM_BLAST_div_HB                       0.117520\n",
       "HB_mul_PLT                            0.104916\n",
       "HB_div_num_trisomies                  0.103098\n",
       "PLT_add_total_mitoses                 0.060907\n",
       "                                        ...   \n",
       "num_inversions_add_num_monosomies     0.000000\n",
       "BM_BLAST_add_num_additions            0.000000\n",
       "BM_BLAST_add_num_trisomies            0.000000\n",
       "num_additions_add_num_trisomies       0.000000\n",
       "num_deletions_add_complexity_score    0.000000\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BM_BLAST_add_WBC</th>\n",
       "      <th>BM_BLAST_div_HB</th>\n",
       "      <th>HB_mul_PLT</th>\n",
       "      <th>HB_div_num_trisomies</th>\n",
       "      <th>PLT_div_num_monosomies</th>\n",
       "      <th>HB_div_num_monosomies</th>\n",
       "      <th>PLT_add_total_mitoses</th>\n",
       "      <th>PLT_div_complexity_score</th>\n",
       "      <th>BM_BLAST_add_MONOCYTES</th>\n",
       "      <th>BM_BLAST_add_ANC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.50</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>1365.0</td>\n",
       "      <td>9.100000e+06</td>\n",
       "      <td>1.500000e+08</td>\n",
       "      <td>9.100000e+06</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.500000e+08</td>\n",
       "      <td>4.050</td>\n",
       "      <td>3.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.00</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>495.0</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>4.500000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.500000e+07</td>\n",
       "      <td>15.000</td>\n",
       "      <td>17.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.70</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>910.8</td>\n",
       "      <td>6.900000e+06</td>\n",
       "      <td>1.320000e+08</td>\n",
       "      <td>6.900000e+06</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1.319999e+02</td>\n",
       "      <td>6.170</td>\n",
       "      <td>6.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>1.779998e+02</td>\n",
       "      <td>9.999990e+00</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.780000e+08</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>4.999998e+00</td>\n",
       "      <td>2.649999e+01</td>\n",
       "      <td>4.999998e+00</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.766666e+01</td>\n",
       "      <td>10.000</td>\n",
       "      <td>11.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>5.80</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>606.8</td>\n",
       "      <td>7.400000e+06</td>\n",
       "      <td>8.200000e+07</td>\n",
       "      <td>7.400000e+06</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8.199992e+01</td>\n",
       "      <td>3.220</td>\n",
       "      <td>4.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>11.90</td>\n",
       "      <td>0.707071</td>\n",
       "      <td>2316.6</td>\n",
       "      <td>9.900000e+06</td>\n",
       "      <td>2.340000e+08</td>\n",
       "      <td>9.900000e+06</td>\n",
       "      <td>254.0</td>\n",
       "      <td>2.339998e+02</td>\n",
       "      <td>7.340</td>\n",
       "      <td>9.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>3.30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1896.6</td>\n",
       "      <td>1.090000e+07</td>\n",
       "      <td>1.740000e+08</td>\n",
       "      <td>1.090000e+07</td>\n",
       "      <td>194.0</td>\n",
       "      <td>1.739998e+02</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>8.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000.9</td>\n",
       "      <td>1.070000e+07</td>\n",
       "      <td>1.870000e+08</td>\n",
       "      <td>1.070000e+07</td>\n",
       "      <td>207.0</td>\n",
       "      <td>1.870000e+08</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>5.31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1702.4</td>\n",
       "      <td>1.119999e+01</td>\n",
       "      <td>1.520000e+08</td>\n",
       "      <td>1.120000e+07</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.520000e+08</td>\n",
       "      <td>0.796</td>\n",
       "      <td>2.283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2221 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BM_BLAST_add_WBC  BM_BLAST_div_HB  HB_mul_PLT  HB_div_num_trisomies  \\\n",
       "0                 6.50         0.329670      1365.0          9.100000e+06   \n",
       "1                19.00         1.363636       495.0          1.100000e+07   \n",
       "2                 8.70         0.869565       910.8          6.900000e+06   \n",
       "3                 4.00         0.200000      1780.0          1.000000e+07   \n",
       "4                12.00         1.000000       530.0          4.999998e+00   \n",
       "...                ...              ...         ...                   ...   \n",
       "2216              5.80         0.405405       606.8          7.400000e+06   \n",
       "2217             11.90         0.707071      2316.6          9.900000e+06   \n",
       "2218              3.30         0.000000      1896.6          1.090000e+07   \n",
       "2219              8.00         0.000000      2000.9          1.070000e+07   \n",
       "2220              5.31         0.000000      1702.4          1.119999e+01   \n",
       "\n",
       "      PLT_div_num_monosomies  HB_div_num_monosomies  PLT_add_total_mitoses  \\\n",
       "0               1.500000e+08           9.100000e+06                  150.0   \n",
       "1               4.500000e+07           1.100000e+07                   65.0   \n",
       "2               1.320000e+08           6.900000e+06                  162.0   \n",
       "3               1.779998e+02           9.999990e+00                  197.0   \n",
       "4               2.649999e+01           4.999998e+00                   78.0   \n",
       "...                      ...                    ...                    ...   \n",
       "2216            8.200000e+07           7.400000e+06                  102.0   \n",
       "2217            2.340000e+08           9.900000e+06                  254.0   \n",
       "2218            1.740000e+08           1.090000e+07                  194.0   \n",
       "2219            1.870000e+08           1.070000e+07                  207.0   \n",
       "2220            1.520000e+08           1.120000e+07                  172.0   \n",
       "\n",
       "      PLT_div_complexity_score  BM_BLAST_add_MONOCYTES  BM_BLAST_add_ANC  \n",
       "0                 1.500000e+08                   4.050             3.840  \n",
       "1                 4.500000e+07                  15.000            17.000  \n",
       "2                 1.319999e+02                   6.170             6.945  \n",
       "3                 1.780000e+08                   2.000             3.000  \n",
       "4                 1.766666e+01                  10.000            11.000  \n",
       "...                        ...                     ...               ...  \n",
       "2216              8.199992e+01                   3.220             4.400  \n",
       "2217              2.339998e+02                   7.340             9.890  \n",
       "2218              1.739998e+02                   0.200             1.730  \n",
       "2219              1.870000e+08                   2.000             4.080  \n",
       "2220              1.520000e+08                   0.796             2.283  \n",
       "\n",
       "[2221 rows x 10 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(itertools.combinations(X.columns, 2)))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "top_features = X_train.corrwith(y_train[\"time\"]).abs().nlargest(100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PLT', 'BM_BLAST', 'HB', 'num_monosomies', 'complexity_score',\n",
       "       'num_trisomies', 'num_additions', 'num_deletions', 'num_translocations',\n",
       "       'num_subclones', 'MONOCYTES', 'WBC', 'sex', 'num_duplications', 'ANC',\n",
       "       'avg_chromosomes', 'num_inversions', 'total_mitoses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
