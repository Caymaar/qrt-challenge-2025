{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utilities import predict_and_save, split_data, get_method_name, score_method\n",
    "from src.preprocess import process_missing_values, main_preprocess, create_entity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Régler le logger de Featuretools au niveau ERROR\n",
    "logging.getLogger('featuretools.entityset').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\".*Ill-conditioned matrix.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL = {\n",
    "    \"cox\": {\"run\": True, \"save\":False, \"shap\": False},\n",
    "    \"xgb\": {\"run\": True, \"save\":True, \"shap\": False},\n",
    "    \"lgbm\": {\"run\": True, \"save\":False, \"shap\": False},\n",
    "    \"rsf\": {\"run\": True, \"save\":False, \"shap\": False}\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    \"size\": 0.7,\n",
    "    \"clinical\": [\"CYTOGENETICS\"],#[\"CYTOGENETICS\"], # Possible: [\"CYTOGENETICS\", \"HB/PLT\", \"logMONOCYTES\", \"logWBC\", \"logANC\"] [\"BM_BLAST+WBC\", \"BM_BLAST/HB\", \"HB*PLT\", \"HB/num_trisomies\"]\n",
    "    \"molecular\": [],#[\"END-START\"], # Possible: [\"GENE\", \"EFFECT\", \"ALT\", \"REF\", \"END-START\"]\n",
    "    \"merge\": [], # Possible: [\"featuretools\", \"gpt\"]\n",
    "    \"additional\": [\n",
    "        #['cadd', 'phred'],\n",
    "        # ['cadd', 'rawscore'],\n",
    "        # # ['cadd', 'consequence'],\n",
    "        # # ['cadd', 'bstatistic'],\n",
    "        # # ['cadd', 'gerp', 'n'],\n",
    "        # ['cadd', 'phast_cons', 'mammalian'],\n",
    "        # ['cadd', 'phylop', 'mammalian'],\n",
    "        # ['snpeff', 'putative_impact'],\n",
    "        # # ['snpeff', 'rank'],\n",
    "        # # ['snpeff', 'total'],\n",
    "         #['cadd', 'exon'],\n",
    "        # # ['cadd', 'cds', 'rel_cds_pos']\n",
    "        ],\n",
    "    \"xgb\": {\n",
    "        'max_depth': 2,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 450,\n",
    "        'subsample': 0.55,\n",
    "        'max_features': 'sqrt',\n",
    "        'random_state': 26\n",
    "    },\n",
    "    \"lgbm\": {\n",
    "        'max_depth': 2,\n",
    "        'learning_rate': 0.05,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    \"rsf\": {\n",
    "    'n_estimators':300,  # Nombre d'arbres dans la forêt\n",
    "    'max_depth':2,\n",
    "    #'min_samples_split':60,  # Nombre minimum d'échantillons requis pour splitter un nœud\n",
    "    #'min_samples_leaf':40,  # Nombre minimum d'échantillons par feuille\n",
    "    'max_features':None,  # Sélection aléatoire des features\n",
    "    'n_jobs':-1,  # Utilisation de tous les cœurs disponibles\n",
    "    }\n",
    "}\n",
    "\n",
    "##############################################\n",
    "# Define the methods used for training\n",
    "##############################################\n",
    "\n",
    "size_method = get_method_name(\"size\", PARAMS)\n",
    "clinical_method = get_method_name(\"clinical\", PARAMS)\n",
    "molecular_method = get_method_name(\"molecular\", PARAMS)\n",
    "merge_method = get_method_name(\"merge\", PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess, Handling missing values, Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_entity(PARAMS)\n",
    "data = main_preprocess(data, PARAMS)\n",
    "X, X_eval, y = split_data(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 - PARAMS['size']), random_state=42)\n",
    "X_train, X_test, X_eval = process_missing_values(X_train, X_test, X_eval, method=\"impute\", strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.report import EDAReport\n",
    "\n",
    "df_analyze = pd.concat([pd.DataFrame(X_train, columns=X.columns), pd.DataFrame(y_train, columns=[\"event\", \"time\"])],axis=1)\n",
    "bool_cols = df_analyze.select_dtypes(include=['bool']).columns\n",
    "df_analyze[bool_cols] = df_analyze[bool_cols].astype(int)\n",
    "\n",
    "ede = EDAReport(df_analyze, target_variables=[\"event\", \"time\"])\n",
    "ede.generate_report()\n",
    "ede.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a CoxPH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoxPHSurvivalAnalysis Model Concordance Index IPCW on train: 0.684\n",
      "CoxPHSurvivalAnalysis Model Concordance Index IPCW on test: 0.671\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL[\"cox\"][\"run\"]:\n",
    "    # Initialize and train the Cox Proportional Hazards model\n",
    "    cox = CoxPHSurvivalAnalysis()    \n",
    "    cox.fit(X_train, y_train)\n",
    "    cox_score_method = score_method(cox, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Predict and save the results\n",
    "    if GLOBAL[\"cox\"][\"save\"]:\n",
    "        predict_and_save(X_eval, cox, method=f\"{size_method}-{cox_score_method}-{clinical_method}-{molecular_method}-{merge_method}\")\n",
    "\n",
    "    if GLOBAL[\"cox\"][\"shap\"]:\n",
    "        from src.report import ShapReport\n",
    "\n",
    "        if not isinstance(X_train, pd.DataFrame):\n",
    "            X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "\n",
    "        report_cox = ShapReport(model=cox, X_train=X_train, predict_function=cox.predict)\n",
    "        report_cox.generate_report(output_html=f\"report/shap/shap_{cox.__class__.__name__}_{size_method}-{cox_score_method}-{clinical_method}-{molecular_method}-{merge_method}.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_lgb = X_train  # Features for training\n",
    "# y_train_transformed = y_train['time']\n",
    "\n",
    "# # Create LightGBM dataset\n",
    "# train_dataset = lgb.Dataset(X_train_lgb, label=y_train_transformed)\n",
    "\n",
    "# # Train the LightGBM model\n",
    "# model = lgb.train(params=PARAMS['lgbm'], train_set=train_dataset)\n",
    "\n",
    "# # Evaluate the model using Concordance Index IPCW\n",
    "# train_ci_ipcw = concordance_index_ipcw(y_train, y_train, -model.predict(X_train), tau=7)[0]\n",
    "# test_ci_ipcw = concordance_index_ipcw(y_train, y_test, -model.predict(X_test), tau=7)[0]\n",
    "# print(f\"LightGBM Survival Model Concordance Index IPCW on train: {train_ci_ipcw:.3f}\")\n",
    "# print(f\"LightGBM Survival Model Concordance Index IPCW on test: {test_ci_ipcw:.3f}\")\n",
    "# lightgbm_score_method = f\"score_{train_ci_ipcw:.3f}_{test_ci_ipcw:.3f}\"\n",
    "\n",
    "# # Predict and save the results\n",
    "# if GLOBAL[\"save_lgbm\"]:\n",
    "#     predict_and_save(X_eval, model, method=f\"{size_method}-{lightgbm_score_method}-{clinical_method}-{molecular_method}-{merge_method}-{PARAMS['xgb']['max_depth']}_lr{PARAMS['xgb']['learning_rate']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Gradient Boosting Survival Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingSurvivalAnalysis Model Concordance Index IPCW on train: 0.723\n",
      "GradientBoostingSurvivalAnalysis Model Concordance Index IPCW on test: 0.680\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL[\"xgb\"][\"run\"]:\n",
    "    xgb_params_method = \"_\".join([(str(key) + \"=\" + str(PARAMS['xgb'][key])) for key in PARAMS['xgb'].keys()])\n",
    "\n",
    "    xgb = GradientBoostingSurvivalAnalysis(**PARAMS['xgb'])\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgboost_score_method = score_method(xgb, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    if GLOBAL[\"xgb\"][\"save\"]:\n",
    "        predict_and_save(X_eval, xgb, method=f\"{size_method}-{xgboost_score_method}--{molecular_method}-{merge_method}-{xgb_params_method}\")\n",
    "\n",
    "    if GLOBAL[\"xgb\"][\"shap\"]:\n",
    "        from src.report import ShapReport\n",
    "\n",
    "        if not isinstance(X_train, pd.DataFrame):\n",
    "            X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "\n",
    "        report_xgb = ShapReport(model=xgb, X_train=X_train, predict_function=xgb.predict)\n",
    "        report_xgb.generate_report(output_html=f\"report/shap/shap_{xgb.__class__.__name__}_{size_method}-{xgboost_score_method}--{molecular_method}-{merge_method}-{xgb_params_method}.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Random Survival Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSurvivalForest Model Concordance Index IPCW on train: 0.668\n",
      "RandomSurvivalForest Model Concordance Index IPCW on test: 0.661\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if GLOBAL[\"rsf\"][\"run\"]:\n",
    "\n",
    "    rsf_params_method = \"_\".join([(str(key) + \"=\" + str(PARAMS['rsf'][key])) for key in PARAMS['rsf'].keys()])\n",
    "\n",
    "    rsf = RandomSurvivalForest(**PARAMS[\"rsf\"], random_state=42)\n",
    "    rsf.fit(X_train, y_train)\n",
    "    rsf_score_method = score_method(rsf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    if GLOBAL[\"rsf\"][\"save\"]:\n",
    "        predict_and_save(X_eval, rsf, method=f\"{size_method}-{rsf_score_method}-{clinical_method}-{molecular_method}-{merge_method}-{rsf_params_method}\")\n",
    "\n",
    "    if GLOBAL[\"rsf\"][\"shap\"]:\n",
    "        from src.report import ShapReport\n",
    "\n",
    "        if not isinstance(X_train, pd.DataFrame):\n",
    "            X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "\n",
    "        report_rsf = ShapReport(model=rsf, X_train=X_train, predict_function=rsf.predict)\n",
    "        report_rsf.generate_report(output_html=f\"report/shap/shap_{rsf.__class__.__name__}_{size_method}-{rsf_score_method}-{clinical_method}-{molecular_method}-{merge_method}-{rsf_params_method}.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Report (Gradient Boosting as an Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.report import ShapReport\n",
    "\n",
    "shap = ShapReport(model=xgb, X_train=pd.DataFrame(data=X_train, columns=X.columns), predict_function=xgb.predict)\n",
    "shap.generate_report(output_file=f\"report/shap/shap_{xgb.__class__.__name__}_{size_method}-{xgboost_score_method}--{molecular_method}-{merge_method}-{xgb_params_method}.html\")\n",
    "shap.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom second preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.deep import convert_float32, convert_survival_data, score_method_deep\n",
    "\n",
    "X_train_deep, X_test_deep, X_eval_deep = convert_float32(X_train, X_test, X_eval)\n",
    "\n",
    "y_train_deep = convert_survival_data(y_train)\n",
    "y_test_deep = convert_survival_data(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_nodes': [128, 128, 23],  # Augmentation de la capacité du modèle\n",
    "    'out_features': 1,\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.0,              # Vous pouvez tester avec ou sans dropout\n",
    "    'output_bias': False,\n",
    "    'in_features': X_train_deep.shape[1]\n",
    "}\n",
    "params_CoxHP = {\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 0.0005,              # Taux d'apprentissage légèrement réduit\n",
    "    \"epochs\": 512,             # Augmentation du nombre d'époques\n",
    "    \"verbose\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a CoxHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.models import CoxPH\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(4)\n",
    "\n",
    "params['in_features'] = X_train_deep.shape[1]\n",
    "\n",
    "net = tt.practical.MLPVanilla(**params)\n",
    "model = CoxPH(net, tt.optim.Adam)\n",
    "\n",
    "model.optimizer.set_lr(params_CoxHP['lr'])\n",
    "\n",
    "batch_size = params_CoxHP['batch_size']\n",
    "\n",
    "lrfinder = model.lr_finder(X_train_deep, y_train_deep, batch_size, tolerance=10)\n",
    "_ = lrfinder.plot()\n",
    "\n",
    "epochs = params_CoxHP['epochs']\n",
    "callbacks = [tt.callbacks.EarlyStopping()]\n",
    "verbose = params_CoxHP['verbose']\n",
    "\n",
    "log = model.fit(X_train_deep, y_train_deep, batch_size, epochs, callbacks, verbose,\n",
    "                val_data=(X_test_deep, y_test_deep), val_batch_size=batch_size)\n",
    "\n",
    "_ = log.plot()\n",
    "\n",
    "print(\"LogLikehood: \", model.partial_log_likelihood(*(X_test_deep, y_test_deep)).mean())\n",
    "\n",
    "score_method_deep(model, X_train_deep, X_test_deep, y_train, y_test, reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.deep import predict_and_save_deep\n",
    "\n",
    "predict_and_save_deep(X_eval_deep, model, method=\"deepCoxHP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
