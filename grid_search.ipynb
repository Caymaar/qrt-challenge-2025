{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 15:13:42,357 featuretools - WARNING    While loading primitives via \"premium_primitives\" entry point, ignored primitive \"DiversityScore\" from \"premium_primitives.diversity_score\" because a primitive with that name already exists in \"nlp_primitives.diversity_score\"\n",
      "2025-03-23 15:13:42,357 featuretools - WARNING    While loading primitives via \"premium_primitives\" entry point, ignored primitive \"LSA\" from \"premium_primitives.lsa\" because a primitive with that name already exists in \"nlp_primitives.lsa\"\n",
      "2025-03-23 15:13:42,358 featuretools - WARNING    While loading primitives via \"premium_primitives\" entry point, ignored primitive \"MeanCharactersPerSentence\" from \"premium_primitives.mean_characters_per_sentence\" because a primitive with that name already exists in \"nlp_primitives.mean_characters_per_sentence\"\n",
      "2025-03-23 15:13:42,358 featuretools - WARNING    While loading primitives via \"premium_primitives\" entry point, ignored primitive \"NumberOfSentences\" from \"premium_primitives.number_of_sentences\" because a primitive with that name already exists in \"nlp_primitives.number_of_sentences\"\n",
      "2025-03-23 15:13:42,359 featuretools - WARNING    While loading primitives via \"premium_primitives\" entry point, ignored primitive \"PartOfSpeechCount\" from \"premium_primitives.part_of_speech_count\" because a primitive with that name already exists in \"nlp_primitives.part_of_speech_count\"\n",
      "2025-03-23 15:13:42,359 featuretools - WARNING    While loading primitives via \"premium_primitives\" entry point, ignored primitive \"PolarityScore\" from \"premium_primitives.polarity_score\" because a primitive with that name already exists in \"nlp_primitives.polarity_score\"\n",
      "2025-03-23 15:13:42,360 featuretools - WARNING    While loading primitives via \"premium_primitives\" entry point, ignored primitive \"StopwordCount\" from \"premium_primitives.stopword_count\" because a primitive with that name already exists in \"nlp_primitives.stopword_count\"\n",
      "Index(['gene_ASXL1', 'gene_BCOR', 'gene_BCORL1', 'gene_BRCC3', 'gene_CBL',\n",
      "       'gene_CEBPA', 'gene_CSF3R', 'gene_CTCF', 'gene_CUX1', 'gene_DDX41',\n",
      "       ...\n",
      "       'SKEW(molecular.START)', 'SKEW(molecular.VAF)', 'STD(molecular.DEPTH)',\n",
      "       'STD(molecular.END)', 'STD(molecular.START)', 'STD(molecular.VAF)',\n",
      "       'SUM(molecular.DEPTH)', 'SUM(molecular.END)', 'SUM(molecular.START)',\n",
      "       'SUM(molecular.VAF)'],\n",
      "      dtype='object', length=102)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres : {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 450, 'subsample': 0.55}\n",
      "Meilleur C-index : 0.7153507201792274\n"
     ]
    }
   ],
   "source": [
    "from src.utilities import split_data\n",
    "from src.preprocess import process_missing_values, main_preprocess, create_entity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "import lightgbm as lgb\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Régler le logger de Featuretools au niveau ERROR\n",
    "logging.getLogger('featuretools.entityset').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\".*Ill-conditioned matrix.*\")\n",
    "\n",
    "GLOBAL = {\n",
    "    \"cox\": {\"run\": False, \"save\":False, \"shap\": False},\n",
    "    \"xgb\": {\"run\": True, \"save\": False, \"shap\": True},\n",
    "    \"lgbm\": {\"run\": False, \"save\":False, \"shap\": False},\n",
    "    \"rsf\": {\"run\": False, \"save\":False, \"shap\": False}\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    \"EDA\" : False, \n",
    "    \"size\": 0.7,\n",
    "    \"impute\": {\"strategy\": \"median\", \"sex\": False},\n",
    "    #\"outliers\": {\"threshold\": 0.01, \"multiplier\": 1.5},\n",
    "    \"clinical\": [\"CYTOGENETICS\"],#[\"CYTOGENETICS\"], # Possible: [\"CYTOGENETICS\", \"HB/PLT\", \"logMONOCYTES\", \"logWBC\", \"logANC\"] [\"BM_BLAST+WBC\", \"BM_BLAST/HB\", \"HB*PLT\", \"HB/num_trisomies\"]\n",
    "    \"molecular\": [\"GENE\"],#[\"END-START\"], # Possible: [\"GENE\", \"EFFECT\", \"ALT\", \"REF\", \"END-START\"]\n",
    "    \"merge\": [\"featuretools\", \"gpt\"], # Possible: [\"featuretools\", \"gpt\"]\n",
    "    \"additional\": [\n",
    "        #['cadd', 'phred'],\n",
    "        # ['cadd', 'rawscore'],\n",
    "        # # ['cadd', 'consequence'],\n",
    "        # # ['cadd', 'bstatistic'],\n",
    "        # # ['cadd', 'gerp', 'n'],\n",
    "        # ['cadd', 'phast_cons', 'mammalian'],\n",
    "        # ['cadd', 'phylop', 'mammalian'],\n",
    "        # ['snpeff', 'putative_impact'],\n",
    "        # # ['snpeff', 'rank'],\n",
    "        # # ['snpeff', 'total'],\n",
    "         #['cadd', 'exon'],\n",
    "        # # ['cadd', 'cds', 'rel_cds_pos']\n",
    "        ],\n",
    "    \"xgb\": {\n",
    "        'loss': 'coxph',\n",
    "        'max_depth': 2,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 335,\n",
    "        'subsample': 0.55,\n",
    "        'max_features': \"sqrt\",\n",
    "        'min_samples_split': 3,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_weight_fraction_leaf': 0,\n",
    "        'min_impurity_decrease': 0,\n",
    "        'dropout_rate': 0,\n",
    "        'warm_start': False,\n",
    "        'ccp_alpha': 0,\n",
    "        'random_state': 126\n",
    "    },\n",
    "    \"lgbm\": {\n",
    "        'max_depth': 2,\n",
    "        'learning_rate': 0.05,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    \"rsf\": {\n",
    "    'n_estimators':200,  # Nombre d'arbres dans la forêt\n",
    "    'max_depth':None,\n",
    "    'min_samples_split':50,  # Nombre minimum d'échantillons requis pour splitter un nœud\n",
    "    'min_samples_leaf':20,  # Nombre minimum d'échantillons par feuille\n",
    "    'max_features':'sqrt',  # Sélection aléatoire des features\n",
    "    'n_jobs':-1,  # Utilisation de tous les cœurs disponibles\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "data = create_entity(PARAMS)\n",
    "data = main_preprocess(data, PARAMS)\n",
    "X, X_eval, y = split_data(data)\n",
    "\n",
    "print(X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 - PARAMS['size']), random_state=42)\n",
    "X_train, X_test, X_eval = process_missing_values(X_train, X_test, X_eval, X.columns, **PARAMS[\"impute\"])\n",
    "\n",
    "# Définition de la grille des hyperparamètres\n",
    "param_grid = {\n",
    "    'max_depth': [2],\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [450],\n",
    "    'subsample': [0.55],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "kfold_params = {\n",
    "    'n_splits': 5,\n",
    "    'shuffle': True,\n",
    "    'random_state': 26\n",
    "}\n",
    "\n",
    "# Configuration du KFold (5 folds)\n",
    "cv = KFold(**kfold_params)\n",
    "\n",
    "# Initialisation du modèle\n",
    "model = GradientBoostingSurvivalAnalysis(random_state=26)\n",
    "\n",
    "\n",
    "\n",
    "# Fonction de scoring basée sur le concordance index censored\n",
    "def cindex_scorer(y_true, y_pred):\n",
    "    return concordance_index_ipcw(y_true, y_true, y_pred, tau=7)[0]\n",
    "\n",
    "# Spécifiez needs_estimator=True pour que make_scorer transmette (estimator, X, y) à votre fonction\n",
    "scorer = make_scorer(cindex_scorer, greater_is_better=True)\n",
    "\n",
    "\n",
    "# Configuration du GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Lancement de la recherche\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "print(\"Meilleur C-index :\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7058 ± 0.0319\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7063 ± 0.0321\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7065 ± 0.0328\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7023 ± 0.0313\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7023 ± 0.0316\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7019 ± 0.0309\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7123 ± 0.0306\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7126 ± 0.0308\n",
      "Paramètres: {'learning_rate': 0.049, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7110 ± 0.0296\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7062 ± 0.0320\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7062 ± 0.0327\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7064 ± 0.0326\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7023 ± 0.0310\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7029 ± 0.0314\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7017 ± 0.0309\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7125 ± 0.0299\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7125 ± 0.0310\n",
      "Paramètres: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7116 ± 0.0299\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7061 ± 0.0320\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7068 ± 0.0322\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7064 ± 0.0328\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7024 ± 0.0313\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7032 ± 0.0315\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7019 ± 0.0307\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.55} => Score moyen: 0.7130 ± 0.0305\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.54} => Score moyen: 0.7129 ± 0.0315\n",
      "Paramètres: {'learning_rate': 0.051, 'max_depth': 2, 'max_features': None, 'n_estimators': 100, 'subsample': 0.56} => Score moyen: 0.7120 ± 0.0294\n"
     ]
    }
   ],
   "source": [
    "for i, params in enumerate(grid_search.cv_results_['params']):\n",
    "    mean_score = grid_search.cv_results_['mean_test_score'][i]\n",
    "    std_score = grid_search.cv_results_['std_test_score'][i]\n",
    "    print(f\"Paramètres: {params} => Score moyen: {mean_score:.4f} ± {std_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
